{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consider four possible models for predicting house prices:\n",
    "\n",
    "Using only the size and number of rooms.\n",
    "Using size, number of rooms, and building type.\n",
    "Using size and building type, and their interaction.\n",
    "Using a 5-degree polynomial on size, a 5-degree polynomial on number of rooms, and also building type.\n",
    "Set up a pipeline for each of these four models.\n",
    "\n",
    "Then, get predictions on the test set for each of your pipelines, and compute the root mean squared error. Which model performed best?\n",
    "\n",
    "Note: You should only use the function train_test_split() one time in your code; that is, we should be predicting on the same test set for all three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "data = (\"/Users/nicoleradovcich/Desktop/MSBA/GSB544/GSB_544/Practice_7/AmesHousing.csv\")\n",
    "ames = pd.read_csv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variables\n",
    "X = ames[['Gr Liv Area', 'TotRms AbvGrd', 'Bldg Type']]\n",
    "y = ames['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the feature transformation\n",
    "numeric_features = ['Gr Liv Area', 'TotRms AbvGrd']\n",
    "categorical_features = ['Bldg Type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using only size and number of rooms\n",
    "pipeline_1 = Pipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numeric_features)\n",
    "        ])),\n",
    "    ('regressor', LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using size, number of rooms, and building type\n",
    "pipeline_2 = Pipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numeric_features),\n",
    "            ('cat', OneHotEncoder(), categorical_features)\n",
    "        ])),\n",
    "    ('regressor', LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using size, building type, and their interaction\n",
    "pipeline_3 = Pipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), ['Gr Liv Area']),\n",
    "            ('cat', OneHotEncoder(), categorical_features)\n",
    "        ])),\n",
    "    ('interaction', PolynomialFeatures(interaction_only=True, include_bias=False)),\n",
    "    ('regressor', LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5-degree polynomial on size, 5-degree polynomial on number of rooms, and building type\n",
    "pipeline_4 = Pipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num_poly', PolynomialFeatures(degree=5), numeric_features),\n",
    "            ('cat', OneHotEncoder(), categorical_features)\n",
    "        ])),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of pipelines\n",
    "pipelines = [pipeline_1, pipeline_2, pipeline_3, pipeline_4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Model 1: 61928.53719680032\n",
      "RMSE for Model 2: 59589.20317423357\n",
      "RMSE for Model 3: 58276.726954679834\n",
      "RMSE for Model 4: 61791.588516218035\n",
      "\n",
      "The best-performing model is Model 3 with RMSE of 58276.7270\n",
      "RMSE for Model 1: 61928.53719680032\n",
      "RMSE for Model 2: 59589.20317423357\n",
      "RMSE for Model 3: 58276.726954679834\n",
      "RMSE for Model 4: 61791.588516218035\n",
      "\n",
      "The best-performing model is Model 3 with RMSE of 58276.7270\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store RMSE for each model\n",
    "rmse_results = {}\n",
    "\n",
    "# Fit each pipeline and compute RMSE\n",
    "for i, pipeline in enumerate(pipelines, 1):\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    rmse_results[f'Model {i}'] = rmse\n",
    "    print(f'RMSE for Model {i}: {rmse}')\n",
    "\n",
    "# Identify the best-performing model based on RMSE\n",
    "best_model = min(rmse_results, key=rmse_results.get)\n",
    "print(f\"\\nThe best-performing model is {best_model} with RMSE of {rmse_results[best_model]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Once again consider four modeling options for house price:\n",
    "\n",
    "Using only the size and number of rooms.\n",
    "Using size, number of rooms, and building type.\n",
    "Using size and building type, and their interaction.\n",
    "Using a 5-degree polynomial on size, a 5-degree polynomial on number of rooms, and also building type.\n",
    "Use cross_val_score with the pipelines you made earlier to find the cross-validated root mean squared error for each model.\n",
    "\n",
    "Which do you prefer? Does this agree with your conclusion from earlier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1: Size and Rooms: Cross-validated RMSE = 55806.33\n",
      "Model 2: Size, Rooms, and Bldg Type: Cross-validated RMSE = 54168.08\n",
      "Model 3: Interaction Features: Cross-validated RMSE = 53363.04\n",
      "Model 4: 5-Degree Polynomials: Cross-validated RMSE = 60117.00\n",
      "Model 1: Size and Rooms: Cross-validated RMSE = 55806.33\n",
      "Model 2: Size, Rooms, and Bldg Type: Cross-validated RMSE = 54168.08\n",
      "Model 3: Interaction Features: Cross-validated RMSE = 53363.04\n",
      "Model 4: 5-Degree Polynomials: Cross-validated RMSE = 60117.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming 'ames' is your dataset\n",
    "X = ames[['Gr Liv Area', 'TotRms AbvGrd', 'Bldg Type']]  # Input features\n",
    "y = ames['SalePrice']  # Target variable\n",
    "\n",
    "# Preprocessing for the models\n",
    "\n",
    "# Model 1: Using size and number of rooms\n",
    "model_1 = Pipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), ['Gr Liv Area', 'TotRms AbvGrd'])  # Scaling numeric features\n",
    "        ])),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Model 2: Using size, number of rooms, and building type\n",
    "model_2 = Pipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), ['Gr Liv Area', 'TotRms AbvGrd']),  # Scaling numeric features\n",
    "            ('cat', OneHotEncoder(drop='first'), ['Bldg Type'])  # One-hot encoding for categorical feature\n",
    "        ])),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Model 3: Using size and building type, including interaction terms\n",
    "model_3 = Pipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), ['Gr Liv Area', 'TotRms AbvGrd']),  # Scaling numeric features\n",
    "            ('cat', OneHotEncoder(drop='first'), ['Bldg Type']),  # One-hot encoding for categorical feature\n",
    "        ])),\n",
    "    ('interaction', PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)),  # Interaction terms\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Model 4: Using 5-degree polynomials on size and number of rooms, and building type\n",
    "model_4 = Pipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', PolynomialFeatures(degree=5, include_bias=False), ['Gr Liv Area', 'TotRms AbvGrd']),  # Polynomial features\n",
    "            ('cat', OneHotEncoder(drop='first'), ['Bldg Type'])  # One-hot encoding for categorical feature\n",
    "        ])),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Cross-validation for each model and compute RMSE\n",
    "models = [model_1, model_2, model_3, model_4]\n",
    "model_names = ['Model 1: Size and Rooms', 'Model 2: Size, Rooms, and Bldg Type', 'Model 3: Interaction Features', 'Model 4: 5-Degree Polynomials']\n",
    "\n",
    "cv_rmse_scores = []\n",
    "\n",
    "# Perform cross-validation for each model and compute RMSE\n",
    "for model in models:\n",
    "    cv_rmse = cross_val_score(model, X, y, cv=5, scoring='neg_root_mean_squared_error')  # Negate RMSE for output\n",
    "    cv_rmse_scores.append(np.mean(cv_rmse))  # Take the mean of the RMSE scores\n",
    "\n",
    "# Output results\n",
    "for name, score in zip(model_names, cv_rmse_scores):\n",
    "    print(f\"{name}: Cross-validated RMSE = {-score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which do you prefer? Does this agree with your conclusion from earlier?\n",
    "This agrees with my conclusion from earlier with Model 3 being the best model in both of these. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consider one hundred modeling options for house price:\n",
    "\n",
    "House size, trying degrees 1 through 10\n",
    "Number of rooms, trying degrees 1 through 10\n",
    "Building Type\n",
    "Hint: The dictionary of possible values that you make to give to GridSearchCV will have two elements instead of one.\n",
    "\n",
    "Q1: Which model performed the best?\n",
    "\n",
    "Q2: What downsides do you see of trying all possible model options? How might you go about choosing a smaller number of tuning values to try?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model parameters: {'preprocessor__num__transformer__degree': 3}\n",
      "Best cross-validated RMSE: 53805.94\n",
      "Best model parameters: {'preprocessor__num__transformer__degree': 3}\n",
      "Best cross-validated RMSE: 53805.94\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Assuming 'ames' is your dataset\n",
    "X = ames[['Gr Liv Area', 'TotRms AbvGrd', 'Bldg Type']]  # Input features\n",
    "y = ames['SalePrice']  # Target variable\n",
    "\n",
    "# Define the grid of hyperparameters to try\n",
    "param_grid = {\n",
    "    'preprocessor__num__transformer__degree': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],  # Polynomial degrees for features\n",
    "}\n",
    "\n",
    "# Create a pipeline\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='mean')),  # Handle missing values\n",
    "                ('transformer', PolynomialFeatures())  # Polynomial features for numeric columns\n",
    "            ]), ['Gr Liv Area', 'TotRms AbvGrd']),  # Apply polynomial features\n",
    "            ('cat', OneHotEncoder(drop='first'), ['Bldg Type'])  # One-hot encoding for categorical feature\n",
    "        ])),\n",
    "    ('regressor', LinearRegression())  # Linear regression model\n",
    "])\n",
    "\n",
    "# GridSearchCV with cross-validation\n",
    "grid_search = GridSearchCV(model_pipeline, param_grid, cv=5, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Fit the model and search for the best parameters\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Get the best parameters and the best RMSE score\n",
    "best_params = grid_search.best_params_\n",
    "best_rmse = -grid_search.best_score_\n",
    "\n",
    "print(f\"Best model parameters: {best_params}\")\n",
    "print(f\"Best cross-validated RMSE: {best_rmse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which model performed the best?\n",
    "Model 3 performed the best. \n",
    "\n",
    "## What downsides do you see of trying all possible model options? How might you go about choosing a smaller number of tuning values to try?\n",
    "Some downsides are that it can result in a lot of models to evaluate which can be time consuming when dealing with large datasets or more complex models. I could go about choosing a smaller number of tuning values by testing a smaller range of hyperparameters. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
